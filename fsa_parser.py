"""
–ü–ê–†–°–ï–† –†–ï–ï–°–¢–†–ê –§–°–ê - –ï–î–ò–ù–´–ô –§–ê–ô–õ –†–ï–ó–£–õ–¨–¢–ê–¢–û–í
–í—Å–µ –¥–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω—è—é—Ç—Å—è –≤ –æ–¥–∏–Ω —Ñ–∞–π–ª output.xlsx
"""

import asyncio
import aiohttp
import pandas as pd
import json
import time
import logging
import sys
from pathlib import Path
from typing import List, Dict, Any, Optional
from datetime import datetime
import traceback

# ================= –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø =================
CONFIG = {
    "output_file": "—Ä–µ–µ—Å—Ç—Ä_—Ñ—Å–∞.xlsx",  # –¢–û–õ–¨–ö–û –û–î–ò–ù –§–ê–ô–õ
    "log_file": "fsa_parser.log",
    "concurrency": 10,
    "request_timeout": 30,
    "batch_size": 1000,  # –ß–∞—Å—Ç–æ—Ç–∞ –∞–≤—Ç–æ—Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
    "max_retries": 3,
    "retry_delay": 2,
    "total_records": 38000,
    
    # –¢–æ–∫–µ–Ω –∞–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏
    "auth_token": "Bearer eyJhbGciOiJFZERTQSJ9.eyJpc3MiOiJGQVUgTklBIiwic3ViIjoiYW5vbnltb3VzIiwiZXhwIjoxNzcwMDY5MTA4LCJpYXQiOjE3NzAwNDAzMDh9.NdwC9BJ-rOk16GOq5GX8T1FmY4rpZXA-pfZjuLT3JeCYaZDc_3sIchWivorKJi4TpAF2-hv9ph1SRD7SzcluBA",
}

# ================= –õ–û–ì–ò–†–û–í–ê–ù–ò–ï =================
def setup_logging():
    logger = logging.getLogger("FSAParser")
    logger.setLevel(logging.INFO)
    
    # –§–æ—Ä–º–∞—Ç
    formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
    
    # –§–∞–π–ª
    file_handler = logging.FileHandler(CONFIG["log_file"], encoding='utf-8')
    file_handler.setFormatter(formatter)
    
    # –ö–æ–Ω—Å–æ–ª—å
    console_handler = logging.StreamHandler()
    console_handler.setFormatter(formatter)
    
    logger.addHandler(file_handler)
    logger.addHandler(console_handler)
    
    return logger

logger = setup_logging()

# ================= –ú–ï–ù–ï–î–ñ–ï–† –î–ê–ù–ù–´–• =================
class DataManager:
    """–£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã–º–∏ –∏ –µ–¥–∏–Ω—ã–º —Ñ–∞–π–ª–æ–º"""
    
    def __init__(self, output_file: str):
        self.output_file = Path(output_file)
        self.all_data = []  # –í—Å–µ –¥–∞–Ω–Ω—ã–µ –≤ –ø–∞–º—è—Ç–∏
        self.last_save_time = time.time()
        self.save_interval = 300  # –°–æ—Ö—Ä–∞–Ω—è—Ç—å –∫–∞–∂–¥—ã–µ 5 –º–∏–Ω—É—Ç
        
    def add_data(self, data: List[Dict]):
        """–î–æ–±–∞–≤–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –∏ –∞–≤—Ç–æ—Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ"""
        self.all_data.extend(data)
        
        # –ê–≤—Ç–æ—Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ø—Ä–∏ –Ω–∞–∫–æ–ø–ª–µ–Ω–∏–∏
        if len(self.all_data) % CONFIG["batch_size"] == 0:
            self.save_to_excel()
            logger.info(f"–ê–≤—Ç–æ—Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ: {len(self.all_data)} –∑–∞–ø–∏—Å–µ–π")
        
        # –ê–≤—Ç–æ—Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ø–æ –≤—Ä–µ–º–µ–Ω–∏
        current_time = time.time()
        if current_time - self.last_save_time > self.save_interval:
            self.save_to_excel()
            self.last_save_time = current_time
    
    def save_to_excel(self):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤—Å–µ—Ö –¥–∞–Ω–Ω—ã—Ö –≤ –µ–¥–∏–Ω—ã–π Excel —Ñ–∞–π–ª"""
        if not self.all_data:
            return
        
        try:
            # –°–æ–∑–¥–∞–µ–º DataFrame
            df = pd.DataFrame(self.all_data)
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø–æ—Ä—è–¥–æ–∫ —Å—Ç–æ–ª–±—Ü–æ–≤
            priority_columns = [
                'id_–∫–æ–º–ø–∞–Ω–∏–∏', '—Å—Ç–∞—Ç—É—Å', '—É–Ω–∏–∫–∞–ª—å–Ω—ã–π_–Ω–æ–º–µ—Ä_–∑–∞–ø–∏—Å–∏',
                '–Ω–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ', '—Å–æ–∫—Ä–∞—â–µ–Ω–Ω–æ–µ_–Ω–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ', '–∏–Ω–Ω',
                '—Ç–∏–ø_–∑–∞—è–≤–∏—Ç–µ–ª—è', '–æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–æ–Ω–Ω–æ_–ø—Ä–∞–≤–æ–≤–∞—è_—Ñ–æ—Ä–º–∞',
                '–¥–∞—Ç–∞_–≤–Ω–µ—Å–µ–Ω–∏—è_–≤_—Ä–µ–µ—Å—Ç—Ä', '–≤–∫–ª—é—á–µ–Ω_–≤_–Ω–∞—Ü–∏–æ–Ω–∞–ª—å–Ω—É—é_—á–∞—Å—Ç—å',
                '—Ñ–∏–æ_—Ä—É–∫–æ–≤–æ–¥–∏—Ç–µ–ª—è', '–¥–æ–ª–∂–Ω–æ—Å—Ç—å_—Ä—É–∫–æ–≤–æ–¥–∏—Ç–µ–ª—è',
                '—Ç–µ–ª–µ—Ñ–æ–Ω', 'email', '—Å–∞–π—Ç', '–∞–¥—Ä–µ—Å_–¥–µ—è—Ç–µ–ª—å–Ω–æ—Å—Ç–∏',
                '–¥–∞—Ç–∞_–ø–∞—Ä—Å–∏–Ω–≥–∞'
            ]
            
            # –£–ø–æ—Ä—è–¥–æ—á–∏–≤–∞–µ–º —Å—Ç–æ–ª–±—Ü—ã
            existing_columns = list(df.columns)
            ordered_columns = []
            
            # –°–Ω–∞—á–∞–ª–∞ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–Ω—ã–µ
            for col in priority_columns:
                if col in existing_columns:
                    ordered_columns.append(col)
                    if col in existing_columns:
                        existing_columns.remove(col)
            
            # –ó–∞—Ç–µ–º –æ—Å—Ç–∞–ª—å–Ω—ã–µ
            ordered_columns.extend(sorted(existing_columns))
            df = df[ordered_columns]
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ Excel
            df.to_excel(self.output_file, index=False, engine='openpyxl')
            
            # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ —Å–æ—Ö—Ä–∞–Ω—è–µ–º –∫–∞–∫ CSV
            csv_file = self.output_file.with_suffix('.csv')
            df.to_csv(csv_file, index=False, encoding='utf-8-sig')
            
            logger.info(f"‚úì –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ {len(df)} –∑–∞–ø–∏—Å–µ–π –≤ {self.output_file}")
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è: {e}")
            
            # –≠–∫—Å—Ç—Ä–µ–Ω–Ω–æ–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ JSON
            try:
                json_file = self.output_file.with_suffix('.json')
                with open(json_file, 'w', encoding='utf-8') as f:
                    json.dump(self.all_data, f, ensure_ascii=False, indent=2)
                logger.info(f"–≠–∫—Å—Ç—Ä–µ–Ω–Ω–æ–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ JSON: {json_file}")
            except:
                logger.error("–ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –¥–∞–∂–µ –≤ JSON!")
    
    def get_stats(self):
        """–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –¥–∞–Ω–Ω—ã–º"""
        if not self.all_data:
            return {"total": 0, "unique_ids": 0}
        
        # –£–Ω–∏–∫–∞–ª—å–Ω—ã–µ ID
        unique_ids = set()
        for item in self.all_data:
            if 'id_–∫–æ–º–ø–∞–Ω–∏–∏' in item:
                unique_ids.add(item['id_–∫–æ–º–ø–∞–Ω–∏–∏'])
        
        return {
            "total_records": len(self.all_data),
            "unique_companies": len(unique_ids)
        }

# ================= –ü–ê–†–°–ï–† =================
class FSAParser:
    """–û—Å–Ω–æ–≤–Ω–æ–π –ø–∞—Ä—Å–µ—Ä"""
    
    def __init__(self):
        self.data_manager = DataManager(CONFIG["output_file"])
        self.headers = {
            "accept": "application/json, text/plain, */*",
            "accept-language": "ru-RU,ru;q=0.9",
            "authorization": CONFIG["auth_token"],
            "user-agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36",
            "referer": "https://pub.fsa.gov.ru/ral",
        }
    
    def extract_company_id(self, source: str) -> str:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ ID –∫–æ–º–ø–∞–Ω–∏–∏"""
        if 'pub.fsa.gov.ru' in source:
            parts = source.rstrip('/').split('/')
            for i, part in enumerate(parts):
                if part == 'view' and i + 1 < len(parts):
                    return parts[i + 1]
            return parts[-2] if len(parts) > 2 else source
        return source
    
    def load_company_ids(self) -> List[str]:
        """–ó–∞–≥—Ä—É–∑–∫–∞ ID –∫–æ–º–ø–∞–Ω–∏–π"""
        files = ["company_ids.txt", "links.txt", "ids.txt", "input.txt"]
        
        all_ids = []
        
        for filename in files:
            filepath = Path(filename)
            if filepath.exists():
                try:
                    logger.info(f"–ó–∞–≥—Ä—É–∑–∫–∞ –∏–∑ {filename}")
                    content = filepath.read_text(encoding='utf-8')
                    lines = [line.strip() for line in content.split('\n') if line.strip()]
                    
                    for line in lines:
                        company_id = self.extract_company_id(line)
                        if company_id and company_id.isdigit():
                            all_ids.append(company_id)
                        elif line.isdigit():
                            all_ids.append(line)
                            
                except Exception as e:
                    logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ {filename}: {e}")
        
        # –£–¥–∞–ª—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã
        unique_ids = list(dict.fromkeys(all_ids))
        logger.info(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(unique_ids)} —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö ID")
        
        # –ï—Å–ª–∏ —Ñ–∞–π–ª–æ–≤ –Ω–µ—Ç, —Å–æ–∑–¥–∞–µ–º —Ç–µ—Å—Ç–æ–≤—ã–µ
        if not unique_ids:
            logger.warning("–§–∞–π–ª—ã —Å ID –Ω–µ –Ω–∞–π–¥–µ–Ω—ã. –°–æ–∑–¥–∞—é —Ç–µ—Å—Ç–æ–≤—ã–µ ID 1-100")
            unique_ids = [str(i) for i in range(1, 101)]
        
        return unique_ids
    
    def clean_value(self, value: Any) -> Any:
        """–û—á–∏—Å—Ç–∫–∞ –∑–Ω–∞—á–µ–Ω–∏—è"""
        if value is None:
            return ""
        elif isinstance(value, bool):
            return "–î–∞" if value else "–ù–µ—Ç"
        elif isinstance(value, (dict, list)):
            try:
                return json.dumps(value, ensure_ascii=False, indent=0)
            except:
                return str(value)
        else:
            return value
    
    def safe_get(self, data: Dict, *keys, default=""):
        """–ë–µ–∑–æ–ø–∞—Å–Ω–æ–µ –ø–æ–ª—É—á–µ–Ω–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è"""
        current = data
        for key in keys:
            if isinstance(current, dict) and key in current:
                current = current[key]
            else:
                return default
        return current if current is not None else default
    
    def extract_company_data(self, company_data: Dict, decl_data: Dict) -> Dict[str, Any]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –∫–æ–º–ø–∞–Ω–∏–∏"""
        result = {}
        
        # –ë–∞–∑–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ
        result['id_–∫–æ–º–ø–∞–Ω–∏–∏'] = self.safe_get(company_data, 'id', default="")
        result['—Å—Ç–∞—Ç—É—Å'] = self.clean_value(
            self.safe_get(company_data, 'status') or 
            self.safe_get(decl_data, 'status')
        )
        
        result['—É–Ω–∏–∫–∞–ª—å–Ω—ã–π_–Ω–æ–º–µ—Ä_–∑–∞–ø–∏—Å–∏'] = self.clean_value(
            self.safe_get(company_data, 'accreditationNumber') or
            self.safe_get(decl_data, 'accreditationNumber') or
            self.safe_get(company_data, 'ralNumber')
        )
        
        result['–Ω–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ'] = self.clean_value(
            self.safe_get(company_data, 'fullName') or
            self.safe_get(company_data, 'name') or
            self.safe_get(decl_data, 'organizationName')
        )
        
        result['—Å–æ–∫—Ä–∞—â–µ–Ω–Ω–æ–µ_–Ω–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ'] = self.clean_value(
            self.safe_get(company_data, 'shortName') or
            self.safe_get(company_data, 'abbreviation')
        )
        
        result['–∏–Ω–Ω'] = self.clean_value(self.safe_get(company_data, 'inn'))
        result['–∫–ø–ø'] = self.clean_value(self.safe_get(company_data, 'kpp'))
        result['–æ–≥—Ä–Ω'] = self.clean_value(self.safe_get(company_data, 'ogrn'))
        
        result['—Ç–∏–ø_–∑–∞—è–≤–∏—Ç–µ–ª—è'] = self.clean_value(
            "–Æ—Ä–∏–¥–∏—á–µ—Å–∫–æ–µ –ª–∏—Ü–æ" if self.safe_get(company_data, 'legalForm') in ['–û–û–û', '–ê–û', '–ü–ê–û'] else
            "–ò–Ω–¥–∏–≤–∏–¥—É–∞–ª—å–Ω—ã–π –ø—Ä–µ–¥–ø—Ä–∏–Ω–∏–º–∞—Ç–µ–ª—å" if self.safe_get(company_data, 'legalForm') == '–ò–ü' else
            self.safe_get(company_data, 'legalForm')
        )
        
        result['–æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–æ–Ω–Ω–æ_–ø—Ä–∞–≤–æ–≤–∞—è_—Ñ–æ—Ä–º–∞'] = self.clean_value(
            self.safe_get(company_data, 'legalForm')
        )
        
        result['–¥–∞—Ç–∞_–≤–Ω–µ—Å–µ–Ω–∏—è_–≤_—Ä–µ–µ—Å—Ç—Ä'] = self.clean_value(
            self.safe_get(company_data, 'registrationDate') or
            self.safe_get(decl_data, 'registrationDate')
        )
        
        result['–≤–∫–ª—é—á–µ–Ω_–≤_–Ω–∞—Ü–∏–æ–Ω–∞–ª—å–Ω—É—é_—á–∞—Å—Ç—å'] = self.clean_value(
            self.safe_get(company_data, 'inNationalRegistry', False)
        )
        
        # –ö–æ–Ω—Ç–∞–∫—Ç–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
        result['—Ñ–∏–æ_—Ä—É–∫–æ–≤–æ–¥–∏—Ç–µ–ª—è'] = self.clean_value(
            self.safe_get(company_data, 'director', 'fullName') or
            self.safe_get(decl_data, 'headName')
        )
        
        result['–¥–æ–ª–∂–Ω–æ—Å—Ç—å_—Ä—É–∫–æ–≤–æ–¥–∏—Ç–µ–ª—è'] = self.clean_value(
            self.safe_get(company_data, 'director', 'position') or
            self.safe_get(decl_data, 'headPosition')
        )
        
        result['—Ç–µ–ª–µ—Ñ–æ–Ω'] = self.clean_value(
            self.safe_get(company_data, 'phone') or
            self.safe_get(company_data, 'contactPhone')
        )
        
        result['email'] = self.clean_value(
            self.safe_get(company_data, 'email') or
            self.safe_get(company_data, 'contactEmail')
        )
        
        result['—Å–∞–π—Ç'] = self.clean_value(
            self.safe_get(company_data, 'website') or
            self.safe_get(decl_data, 'website')
        )
        
        result['–∞–¥—Ä–µ—Å_–¥–µ—è—Ç–µ–ª—å–Ω–æ—Å—Ç–∏'] = self.clean_value(
            self.safe_get(company_data, 'address', 'fullAddress') or
            self.safe_get(decl_data, 'activityAddress') or
            self.safe_get(company_data, 'legalAddress')
        )
        
        # –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
        result['–¥–∞—Ç–∞_–ø–∞—Ä—Å–∏–Ω–≥–∞'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        result['–∏—Å—Ç–æ—á–Ω–∏–∫'] = 'https://pub.fsa.gov.ru/ral'
        
        return result
    
    async def fetch_with_retry(self, session, url, params=None, retries=3):
        """–ó–∞–ø—Ä–æ—Å —Å –ø–æ–≤—Ç–æ—Ä–Ω—ã–º–∏ –ø–æ–ø—ã—Ç–∫–∞–º–∏"""
        for attempt in range(retries):
            try:
                async with session.get(url, params=params, timeout=CONFIG["request_timeout"]) as resp:
                    if resp.status == 200:
                        return await resp.json()
                    elif resp.status == 404:
                        return {"_status": "NOT_FOUND"}
                    elif resp.status == 429:
                        wait = (attempt + 1) * 5
                        logger.warning(f"–°–ª–∏—à–∫–æ–º –º–Ω–æ–≥–æ –∑–∞–ø—Ä–æ—Å–æ–≤. –ñ–¥–µ–º {wait} —Å–µ–∫.")
                        await asyncio.sleep(wait)
                        continue
                    else:
                        logger.error(f"HTTP {resp.status} –¥–ª—è {url}")
                        if attempt < retries - 1:
                            await asyncio.sleep(CONFIG["retry_delay"])
                            continue
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–ø—Ä–æ—Å–∞: {e}")
                if attempt < retries - 1:
                    await asyncio.sleep(CONFIG["retry_delay"])
                    continue
        
        return {"_status": "FAILED"}
    
    async def process_company(self, session, company_id, idx, total):
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ –æ–¥–Ω–æ–π –∫–æ–º–ø–∞–Ω–∏–∏"""
        try:
            # –î–∞–Ω–Ω—ã–µ –∫–æ–º–ø–∞–Ω–∏–∏
            company_url = f"https://pub.fsa.gov.ru/api/v1/ral/common/companies/{company_id}"
            company_data = await self.fetch_with_retry(session, company_url)
            
            if not company_data or company_data.get('_status') in ['NOT_FOUND', 'FAILED']:
                return {
                    'id_–∫–æ–º–ø–∞–Ω–∏–∏': company_id,
                    '—Å—Ç–∞—Ç—É—Å': '–ù–µ –Ω–∞–π–¥–µ–Ω–æ',
                    '–¥–∞—Ç–∞_–ø–∞—Ä—Å–∏–Ω–≥–∞': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
                }
            
            # –î–∞–Ω–Ω—ã–µ –¥–µ–∫–ª–∞—Ä–∞—Ü–∏–∏
            decl_data = {"_status": "NO_DATA"}
            accreditation = self.safe_get(company_data, 'accreditation', default={})
            
            if isinstance(accreditation, dict):
                doc_id = accreditation.get('idAccredScopeFile') or accreditation.get('id')
                if doc_id:
                    decl_url = "https://pub.fsa.gov.ru/api/v1/oa/accreditation/declaration/view/"
                    params = {"docId": doc_id, "alType": 5, "validate": "false"}
                    decl_data = await self.fetch_with_retry(session, decl_url, params=params)
            
            # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö
            result = self.extract_company_data(company_data, decl_data)
            result['–æ–±—Ä–∞–±–æ—Ç–∫–∞'] = '–£—Å–ø–µ—à–Ω–æ'
            
            logger.info(f"[{idx}/{total}] –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {result.get('–Ω–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ', company_id)}")
            return result
            
        except Exception as e:
            logger.error(f"[{idx}] –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ {company_id}: {e}")
            return {
                'id_–∫–æ–º–ø–∞–Ω–∏–∏': company_id,
                '—Å—Ç–∞—Ç—É—Å': f'–û—à–∏–±–∫–∞: {str(e)[:100]}',
                '–¥–∞—Ç–∞_–ø–∞—Ä—Å–∏–Ω–≥–∞': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                '–æ–±—Ä–∞–±–æ—Ç–∫–∞': '–û—à–∏–±–∫–∞'
            }
    
    async def run(self):
        """–û—Å–Ω–æ–≤–Ω–æ–π –∑–∞–ø—É—Å–∫"""
        logger.info("=" * 60)
        logger.info("üöÄ –ó–ê–ü–£–°–ö –ü–ê–†–°–ï–†–ê –§–°–ê")
        logger.info(f"üìÅ –í—ã—Ö–æ–¥–Ω–æ–π —Ñ–∞–π–ª: {CONFIG['output_file']}")
        logger.info("=" * 60)
        
        # –ó–∞–≥—Ä—É–∑–∫–∞ ID
        company_ids = self.load_company_ids()
        total = len(company_ids)
        
        if CONFIG["total_records"] > 0:
            company_ids = company_ids[:CONFIG["total_records"]]
            total = min(total, CONFIG["total_records"])
        
        logger.info(f"üìä –í—Å–µ–≥–æ –∑–∞–ø–∏—Å–µ–π: {total}")
        
        start_time = time.time()
        processed = 0
        failed = 0
        
        # –°–æ–∑–¥–∞–µ–º —Å–µ—Å—Å–∏—é
        timeout = aiohttp.ClientTimeout(total=CONFIG["request_timeout"])
        connector = aiohttp.TCPConnector(limit=CONFIG["concurrency"])
        
        async with aiohttp.ClientSession(
            headers=self.headers,
            timeout=timeout,
            connector=connector
        ) as session:
            
            # –°–µ–º–∞—Ñ–æ—Ä –¥–ª—è –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è
            sem = asyncio.Semaphore(CONFIG["concurrency"])
            
            async def process_with_limit(company_id, idx):
                async with sem:
                    return await self.process_company(session, company_id, idx, total)
            
            # –°–æ–∑–¥–∞–µ–º –∑–∞–¥–∞—á–∏
            tasks = []
            for idx, company_id in enumerate(company_ids, 1):
                tasks.append(process_with_limit(company_id, idx))
            
            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º
            batch_results = []
            for idx, task in enumerate(asyncio.as_completed(tasks), 1):
                try:
                    result = await task
                    processed += 1
                    
                    if result.get('–æ–±—Ä–∞–±–æ—Ç–∫–∞') == '–£—Å–ø–µ—à–Ω–æ':
                        batch_results.append(result)
                    else:
                        batch_results.append(result)
                        failed += 1
                    
                    # –î–æ–±–∞–≤–ª—è–µ–º –≤ –º–µ–Ω–µ–¥–∂–µ—Ä –¥–∞–Ω–Ω—ã—Ö
                    if batch_results:
                        self.data_manager.add_data(batch_results)
                        batch_results = []
                    
                    # –ü—Ä–æ–≥—Ä–µ—Å—Å
                    if idx % 100 == 0 or idx == total:
                        elapsed = time.time() - start_time
                        speed = processed / elapsed if elapsed > 0 else 0
                        remaining = (total - processed) / speed if speed > 0 else 0
                        
                        logger.info(
                            f"üìà –ü—Ä–æ–≥—Ä–µ—Å—Å: {processed}/{total} ({processed/total*100:.1f}%) | "
                            f"–°–∫–æ—Ä–æ—Å—Ç—å: {speed:.1f}/—Å–µ–∫ | "
                            f"–û—à–∏–±–æ–∫: {failed} | "
                            f"–û—Å—Ç–∞–ª–æ—Å—å: ~{remaining/60:.0f} –º–∏–Ω"
                        )
                        
                except Exception as e:
                    logger.error(f"–û—à–∏–±–∫–∞ –≤ –∑–∞–¥–∞—á–µ {idx}: {e}")
                    failed += 1
        
        # –§–∏–Ω–∞–ª—å–Ω–æ–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
        if batch_results:
            self.data_manager.add_data(batch_results)
        self.data_manager.save_to_excel()
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        end_time = time.time()
        total_time = end_time - start_time
        stats = self.data_manager.get_stats()
        
        logger.info("=" * 60)
        logger.info("‚úÖ –ü–ê–†–°–ò–ù–ì –ó–ê–í–ï–†–®–ï–ù!")
        logger.info(f"üìä –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {processed} –∑–∞–ø–∏—Å–µ–π")
        logger.info(f"‚ùå –û—à–∏–±–æ–∫: {failed}")
        logger.info(f"‚è±Ô∏è  –í—Ä–µ–º—è: {total_time/60:.1f} –º–∏–Ω—É—Ç")
        logger.info(f"üöÄ –°–∫–æ—Ä–æ—Å—Ç—å: {processed/total_time:.1f} –∑–∞–ø–∏—Å–µ–π/—Å–µ–∫")
        logger.info(f"üíæ –§–∞–π–ª: {CONFIG['output_file']} ({stats['total_records']} –∑–∞–ø–∏—Å–µ–π)")
        logger.info("=" * 60)
        
        # –û—Ç—á–µ—Ç
        report = f"""
–û–¢–ß–ï–¢ –û –ü–ê–†–°–ò–ù–ì–ï –§–°–ê
{'=' * 40}
–î–∞—Ç–∞: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
–í—Å–µ–≥–æ –∑–∞–ø–∏—Å–µ–π: {total}
–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {processed}
–û—à–∏–±–æ–∫: {failed}
–í—Ä–µ–º—è: {total_time/60:.1f} –º–∏–Ω—É—Ç
–°–∫–æ—Ä–æ—Å—Ç—å: {processed/total_time:.1f} –∑–∞–ø–∏—Å–µ–π/—Å–µ–∫
–í—ã—Ö–æ–¥–Ω–æ–π —Ñ–∞–π–ª: {CONFIG['output_file']}
–ó–∞–ø–∏—Å–µ–π –≤ —Ñ–∞–π–ª–µ: {stats['total_records']}
"""
        
        with open("–æ—Ç—á–µ—Ç.txt", "w", encoding="utf-8") as f:
            f.write(report)

# ================= –£–¢–ò–õ–ò–¢–´ =================
def generate_test_ids():
    """–°–æ–∑–¥–∞–Ω–∏–µ —Ç–µ—Å—Ç–æ–≤—ã—Ö ID"""
    ids = list(range(1, 101))
    with open("company_ids.txt", "w", encoding="utf-8") as f:
        f.write("\n".join(str(i) for i in ids))
    print(f"–°–æ–∑–¥–∞–Ω–æ {len(ids)} —Ç–µ—Å—Ç–æ–≤—ã—Ö ID –≤ company_ids.txt")

def check_api():
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ API"""
    import requests
    
    url = "https://pub.fsa.gov.ru/api/v1/ral/common/companies/1"
    headers = {
        "authorization": CONFIG["auth_token"],
        "user-agent": "Mozilla/5.0"
    }
    
    try:
        response = requests.get(url, headers=headers, timeout=10)
        print(f"API —Å—Ç–∞—Ç—É—Å: {response.status_code}")
        
        if response.status_code == 200:
            data = response.json()
            print(f"–ü—Ä–∏–º–µ—Ä –¥–∞–Ω–Ω—ã—Ö:")
            print(f"  ID: {data.get('id')}")
            print(f"  –ù–∞–∑–≤–∞–Ω–∏–µ: {data.get('fullName')}")
            print(f"  –ò–ù–ù: {data.get('inn')}")
            print(f"  –°—Ç–∞—Ç—É—Å: {data.get('status')}")
        else:
            print(f"–¢–µ–∫—Å—Ç –æ—Ç–≤–µ—Ç–∞: {response.text[:200]}")
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞: {e}")

# ================= –ó–ê–ü–£–°–ö =================
async def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    
    if len(sys.argv) > 1:
        command = sys.argv[1]
        
        if command == "test":
            print("üß™ –¢–ï–°–¢–û–í–´–ô –†–ï–ñ–ò–ú (100 –∑–∞–ø–∏—Å–µ–π)")
            generate_test_ids()
            parser = FSAParser()
            await parser.run()
            
        elif command == "check":
            print("üîç –ü–†–û–í–ï–†–ö–ê API")
            check_api()
            
        elif command == "full":
            print("üöÄ –ü–û–õ–ù–´–ô –ó–ê–ü–£–°–ö")
            parser = FSAParser()
            await parser.run()
            
        elif command == "resume":
            print("üîÑ –ü–†–û–î–û–õ–ñ–ï–ù–ò–ï")
            # –î–ª—è –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏—è –Ω—É–∂–Ω–æ —Å–æ—Ö—Ä–∞–Ω—è—Ç—å –ø—Ä–æ–≥—Ä–µ—Å—Å
            parser = FSAParser()
            await parser.run()
            
        else:
            print("–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ:")
            print("  python fsa_parser.py test    - —Ç–µ—Å—Ç (100 –∑–∞–ø–∏—Å–µ–π)")
            print("  python fsa_parser.py full    - –ø–æ–ª–Ω—ã–π –∑–∞–ø—É—Å–∫")
            print("  python fsa_parser.py check   - –ø—Ä–æ–≤–µ—Ä–∫–∞ API")
            print("  python fsa_parser.py resume  - –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏–µ")
    else:
        # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é - –ø–æ–ª–Ω—ã–π –∑–∞–ø—É—Å–∫
        parser = FSAParser()
        await parser.run()

if __name__ == "__main__":
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        print("\n\n‚èπÔ∏è  –ü–∞—Ä—Å–µ—Ä –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º")
        print("üìÅ –î–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ —Ç–µ–∫—É—â–µ–º —Å–æ—Å—Ç–æ—è–Ω–∏–∏")
    except Exception as e:
        print(f"\n\n‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {e}")
