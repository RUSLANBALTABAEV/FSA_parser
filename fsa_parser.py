"""
–ü–ê–†–°–ï–† –†–ï–ï–°–¢–†–ê –ê–ö–ö–†–ï–î–ò–¢–û–í–ê–ù–ù–´–• –õ–ò–¶ –§–°–ê
–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π —Å–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö —Å pub.fsa.gov.ru/ral
–í–µ—Ä—Å–∏—è 2.0 - –ü–æ–ª–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –≤—Å–µ—Ö —Ä–∞–∑–¥–µ–ª–æ–≤
"""

import asyncio
import aiohttp
import pandas as pd
import json
import time
import logging
from pathlib import Path
from typing import List, Dict, Any, Optional
from datetime import datetime
import traceback
import aiofiles
import sys
import requests
from bs4 import BeautifulSoup

# ================= –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø =================
CONFIG = {
    "log_file": "fsa_parser_full.log",
    "output_excel": "—Ä–µ–µ—Å—Ç—Ä_—Ñ—Å–∞_–ø–æ–ª–Ω—ã–π.xlsx",
    "output_csv": "—Ä–µ–µ—Å—Ç—Ä_—Ñ—Å–∞_–ø–æ–ª–Ω—ã–π.csv",
    "output_json": "—Ä–µ–µ—Å—Ç—Ä_—Ñ—Å–∞_–ø–æ–ª–Ω—ã–π.json",
    "batch_size": 500,          # –°–æ—Ö—Ä–∞–Ω—è—Ç—å –∫–∞–∂–¥—ã–µ 500 –∑–∞–ø–∏—Å–µ–π
    "concurrency": 10,          # –û–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
    "request_timeout": 45,      # –¢–∞–π–º–∞—É—Ç –∑–∞–ø—Ä–æ—Å–∞
    "retry_attempts": 3,        # –ü–æ–ø—ã—Ç–∫–∏ –ø–æ–≤—Ç–æ—Ä–∞
    "retry_delay": 3,           # –ó–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –ø–æ–≤—Ç–æ—Ä–∞–º–∏
    "max_records": 38000,       # –ú–∞–∫—Å–∏–º—É–º –∑–∞–ø–∏—Å–µ–π (0 = –≤—Å–µ)
    "user_agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36",
    
    # –¢–æ–∫–µ–Ω (–º–æ–∂–µ—Ç –ø–æ—Ç—Ä–µ–±–æ–≤–∞—Ç—å—Å—è –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ)
    "auth_token": "Bearer eyJhbGciOiJFZERTQSJ9.eyJpc3MiOiJGQVUgTklBIiwic3ViIjoiYW5vbnltb3VzIiwiZXhwIjoxNzcwMDY5MTA4LCJpYXQiOjE3NzAwNDAzMDh9.NdwC9BJ-rOk16GOq5GX8T1FmY4rpZXA-pfZjuLT3JeCYaZDc_3sIchWivorKJi4TpAF2-hv9ph1SRD7SzcluBA",
}

# ================= –õ–û–ì–ò–†–û–í–ê–ù–ò–ï =================
def setup_logging():
    """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è"""
    logger = logging.getLogger("FSAParser")
    logger.setLevel(logging.INFO)
    
    # –§–æ—Ä–º–∞—Ç—Ç–µ—Ä
    formatter = logging.Formatter(
        '%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        datefmt='%Y-%m-%d %H:%M:%S'
    )
    
    # –§–∞–π–ª–æ–≤—ã–π –æ–±—Ä–∞–±–æ—Ç—á–∏–∫
    file_handler = logging.FileHandler(
        CONFIG["log_file"], 
        encoding='utf-8'
    )
    file_handler.setFormatter(formatter)
    
    # –ö–æ–Ω—Å–æ–ª—å–Ω—ã–π –æ–±—Ä–∞–±–æ—Ç—á–∏–∫
    console_handler = logging.StreamHandler()
    console_handler.setFormatter(formatter)
    
    # –î–æ–±–∞–≤–ª—è–µ–º –æ–±—Ä–∞–±–æ—Ç—á–∏–∫–∏
    logger.addHandler(file_handler)
    logger.addHandler(console_handler)
    
    return logger

logger = setup_logging()

# ================= –ö–õ–ê–°–° –î–õ–Ø –†–ê–ë–û–¢–´ –° API =================
class FSAApiClient:
    """–ö–ª–∏–µ–Ω—Ç –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å API –§–°–ê"""
    
    def __init__(self):
        self.base_url = "https://pub.fsa.gov.ru"
        self.headers = {
            "accept": "application/json, text/plain, */*",
            "accept-language": "ru-RU,ru;q=0.9,en-US;q=0.8,en;q=0.7",
            "authorization": CONFIG["auth_token"],
            "cache-control": "no-cache",
            "pragma": "no-cache",
            "user-agent": CONFIG["user_agent"],
            "referer": f"{self.base_url}/ral",
            "origin": self.base_url,
        }
        self.session = None
        
    async def __aenter__(self):
        """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –≤—Ö–æ–¥ –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç"""
        timeout = aiohttp.ClientTimeout(total=CONFIG["request_timeout"])
        connector = aiohttp.TCPConnector(
            limit=CONFIG["concurrency"],
            limit_per_host=CONFIG["concurrency"],
            force_close=True,
            enable_cleanup_closed=True
        )
        self.session = aiohttp.ClientSession(
            headers=self.headers,
            timeout=timeout,
            connector=connector,
            trust_env=True
        )
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –≤—ã—Ö–æ–¥ –∏–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞"""
        if self.session:
            await self.session.close()
    
    async def fetch_json(self, url: str, params: dict = None) -> Optional[Dict]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ JSON —Å –ø–æ–≤—Ç–æ—Ä–Ω—ã–º–∏ –ø–æ–ø—ã—Ç–∫–∞–º–∏"""
        for attempt in range(CONFIG["retry_attempts"]):
            try:
                async with self.session.get(
                    url, 
                    params=params,
                    ssl=False
                ) as response:
                    
                    if response.status == 200:
                        return await response.json()
                    elif response.status == 404:
                        logger.debug(f"404 –¥–ª—è {url}")
                        return {"_status": "NOT_FOUND"}
                    elif response.status == 429:
                        wait = (attempt + 1) * 10
                        logger.warning(f"429 –°–ª–∏—à–∫–æ–º –º–Ω–æ–≥–æ –∑–∞–ø—Ä–æ—Å–æ–≤. –ñ–¥–µ–º {wait} —Å–µ–∫.")
                        await asyncio.sleep(wait)
                        continue
                    elif response.status >= 500:
                        logger.warning(f"–û—à–∏–±–∫–∞ —Å–µ—Ä–≤–µ—Ä–∞ {response.status}")
                        await asyncio.sleep(CONFIG["retry_delay"])
                        continue
                    else:
                        logger.error(f"HTTP {response.status} –¥–ª—è {url}")
                        if attempt < CONFIG["retry_attempts"] - 1:
                            await asyncio.sleep(CONFIG["retry_delay"])
                            continue
                        return {"_status": f"ERROR_{response.status}"}
                        
            except asyncio.TimeoutError:
                logger.warning(f"–¢–∞–π–º–∞—É—Ç. –ü–æ–ø—ã—Ç–∫–∞ {attempt + 1}/{CONFIG['retry_attempts']}")
                await asyncio.sleep(CONFIG["retry_delay"])
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–ø—Ä–æ—Å–∞: {str(e)[:100]}")
                if attempt < CONFIG["retry_attempts"] - 1:
                    await asyncio.sleep(CONFIG["retry_delay"])
                    continue
        
        logger.error(f"–ù–µ —É–¥–∞–ª–æ—Å—å –≤—ã–ø–æ–ª–Ω–∏—Ç—å –∑–∞–ø—Ä–æ—Å: {url}")
        return {"_status": "FAILED"}
    
    async def get_company_data(self, company_id: str) -> Optional[Dict]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –∫–æ–º–ø–∞–Ω–∏–∏"""
        url = f"{self.base_url}/api/v1/ral/common/companies/{company_id}"
        return await self.fetch_json(url)
    
    async def get_declaration_data(self, doc_id: str) -> Optional[Dict]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –¥–µ–∫–ª–∞—Ä–∞—Ü–∏–∏"""
        url = f"{self.base_url}/api/v1/oa/accreditation/declaration/view/"
        params = {"docId": doc_id, "alType": 5, "validate": "false"}
        return await self.fetch_json(url, params)

# ================= –ö–õ–ê–°–° –î–õ–Ø –ò–ó–í–õ–ï–ß–ï–ù–ò–Ø –î–ê–ù–ù–´–• =================
class DataExtractor:
    """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∏ –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∏–∑ API"""
    
    @staticmethod
    def safe_get(data: Dict, path: str, default: Any = ""):
        """–ë–µ–∑–æ–ø–∞—Å–Ω–æ–µ –ø–æ–ª—É—á–µ–Ω–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è –ø–æ –ø—É—Ç–∏"""
        if not data or not isinstance(data, dict):
            return default
        
        parts = path.split('.')
        current = data
        
        for part in parts:
            if isinstance(current, dict) and part in current:
                current = current[part]
            else:
                return default
        
        return current if current is not None else default
    
    @staticmethod
    def clean_value(value: Any) -> str:
        """–û—á–∏—Å—Ç–∫–∞ –∑–Ω–∞—á–µ–Ω–∏—è –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è"""
        if value is None:
            return ""
        elif isinstance(value, bool):
            return "–î–∞" if value else "–ù–µ—Ç"
        elif isinstance(value, (dict, list)):
            try:
                return json.dumps(value, ensure_ascii=False, indent=0)
            except:
                return str(value)
        elif isinstance(value, (int, float)):
            return str(value)
        else:
            return str(value).strip()
    
    @classmethod
    def extract_accredited_person(cls, company_data: Dict, decl_data: Dict) -> Dict[str, Any]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö —Ä–∞–∑–¥–µ–ª–∞ '–ê–∫–∫—Ä–µ–¥–∏—Ç–æ–≤–∞–Ω–æ –ª–∏—Ü–æ'"""
        result = {}
        
        # –ë–∞–∑–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ
        result['—Å—Ç–∞—Ç—É—Å'] = cls.clean_value(
            cls.safe_get(company_data, 'status') or 
            cls.safe_get(decl_data, 'status')
        )
        
        result['–¥–∞—Ç–∞_–≤–Ω–µ—Å–µ–Ω–∏—è_–≤_—Ä–µ–µ—Å—Ç—Ä'] = cls.clean_value(
            cls.safe_get(company_data, 'registrationDate') or
            cls.safe_get(decl_data, 'registrationDate')
        )
        
        result['–≤–∫–ª—é—á–µ–Ω_–≤_–Ω–∞—Ü–∏–æ–Ω–∞–ª—å–Ω—É—é_—á–∞—Å—Ç—å'] = cls.clean_value(
            cls.safe_get(company_data, 'inNationalRegistry', False)
        )
        
        result['–Ω–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ_—Å—Ç–∞–Ω–¥–∞—Ä—Ç–∞'] = cls.clean_value(
            cls.safe_get(decl_data, 'standard.name') or
            cls.safe_get(decl_data, 'accreditationStandard')
        )
        
        result['—É–Ω–∏–∫–∞–ª—å–Ω—ã–π_–Ω–æ–º–µ—Ä_–∑–∞–ø–∏—Å–∏'] = cls.clean_value(
            cls.safe_get(company_data, 'accreditationNumber') or
            cls.safe_get(decl_data, 'accreditationNumber') or
            cls.safe_get(company_data, 'ralNumber')
        )
        
        result['–Ω–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ'] = cls.clean_value(
            cls.safe_get(company_data, 'fullName') or
            cls.safe_get(company_data, 'name') or
            cls.safe_get(decl_data, 'organizationName')
        )
        
        result['—Å–æ–∫—Ä–∞—â–µ–Ω–Ω–æ–µ_–Ω–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ'] = cls.clean_value(
            cls.safe_get(company_data, 'shortName') or
            cls.safe_get(company_data, 'abbreviation')
        )
        
        # –†—É–∫–æ–≤–æ–¥—Å—Ç–≤–æ
        result['—Ñ–∏–æ_—Ä—É–∫–æ–≤–æ–¥–∏—Ç–µ–ª—è'] = cls.clean_value(
            cls.safe_get(company_data, 'director.fullName') or
            cls.safe_get(decl_data, 'headName')
        )
        
        result['–¥–æ–ª–∂–Ω–æ—Å—Ç—å_—Ä—É–∫–æ–≤–æ–¥–∏—Ç–µ–ª—è'] = cls.clean_value(
            cls.safe_get(company_data, 'director.position') or
            cls.safe_get(decl_data, 'headPosition')
        )
        
        result['—Ç–µ–ª–µ—Ñ–æ–Ω'] = cls.clean_value(
            cls.safe_get(company_data, 'phone') or
            cls.safe_get(company_data, 'contactPhone')
        )
        
        result['—Ç–µ–ª–µ—Ñ–æ–Ω_—Ä—É–∫–æ–≤–æ–¥–∏—Ç–µ–ª—è'] = cls.clean_value(
            cls.safe_get(company_data, 'director.phone') or
            cls.safe_get(decl_data, 'headPhone')
        )
        
        result['email'] = cls.clean_value(
            cls.safe_get(company_data, 'email') or
            cls.safe_get(company_data, 'contactEmail')
        )
        
        result['—Å–∞–π—Ç'] = cls.clean_value(
            cls.safe_get(company_data, 'website') or
            cls.safe_get(decl_data, 'website')
        )
        
        # –ê–¥—Ä–µ—Å–∞
        addresses = []
        for addr_path in ['address.fullAddress', 'legalAddress', 'activityAddress']:
            addr = cls.safe_get(company_data, addr_path) or cls.safe_get(decl_data, addr_path)
            if addr and addr not in addresses:
                addresses.append(addr)
        
        result['–∞–¥—Ä–µ—Å_–¥–µ—è—Ç–µ–ª—å–Ω–æ—Å—Ç–∏'] = cls.clean_value("; ".join(addresses))
        
        # –ì–æ—Å—É–¥–∞—Ä—Å—Ç–≤–µ–Ω–Ω—ã–µ —É—Å–ª—É–≥–∏ (–∏–∑ PDF)
        result['–Ω–æ–º–µ—Ä_–≥–æ—Å_—É—Å–ª—É–≥–∏'] = cls.clean_value(
            cls.safe_get(decl_data, 'stateServiceNumber')
        )
        
        result['–¥–∞—Ç–∞_–≥–æ—Å_—É—Å–ª—É–≥–∏'] = cls.clean_value(
            cls.safe_get(decl_data, 'stateServiceDate')
        )
        
        result['–Ω–æ–º–µ—Ä_—Ä–µ—à–µ–Ω–∏—è'] = cls.clean_value(
            cls.safe_get(decl_data, 'decisionNumber')
        )
        
        result['–¥–∞—Ç–∞_—Ä–µ—à–µ–Ω–∏—è'] = cls.clean_value(
            cls.safe_get(decl_data, 'decisionDate')
        )
        
        # –û–±–ª–∞—Å—Ç—å –∞–∫–∫—Ä–µ–¥–∏—Ç–∞—Ü–∏–∏
        accreditation_scope = cls.safe_get(decl_data, 'accreditationScope', [])
        if isinstance(accreditation_scope, list):
            scope_texts = []
            for scope in accreditation_scope:
                if isinstance(scope, dict):
                    desc = scope.get('description') or scope.get('name') or str(scope)
                    if desc and desc not in scope_texts:
                        scope_texts.append(desc)
            result['–æ–ø–∏—Å–∞–Ω–∏–µ_–æ–±–ª–∞—Å—Ç–∏_–∞–∫–∫—Ä–µ–¥–∏—Ç–∞—Ü–∏–∏'] = cls.clean_value(" | ".join(scope_texts))
        
        return result
    
    @classmethod
    def extract_applicant_data(cls, company_data: Dict) -> Dict[str, Any]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö —Ä–∞–∑–¥–µ–ª–∞ '–ó–∞—è–≤–∏—Ç–µ–ª—å'"""
        result = {}
        
        # –¢–∏–ø –∏ —Ñ–æ—Ä–º–∞
        legal_form = cls.safe_get(company_data, 'legalForm')
        result['—Ç–∏–ø_–∑–∞—è–≤–∏—Ç–µ–ª—è'] = cls.clean_value(
            "–Æ—Ä–∏–¥–∏—á–µ—Å–∫–æ–µ –ª–∏—Ü–æ" if legal_form in ['–û–û–û', '–ó–ê–û', '–û–ê–û', '–ê–û', '–ü–ê–û'] else 
            "–ò–Ω–¥–∏–≤–∏–¥—É–∞–ª—å–Ω—ã–π –ø—Ä–µ–¥–ø—Ä–∏–Ω–∏–º–∞—Ç–µ–ª—å" if legal_form in ['–ò–ü'] else
            legal_form or "–Æ—Ä–∏–¥–∏—á–µ—Å–∫–æ–µ –ª–∏—Ü–æ"
        )
        
        result['–æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–æ–Ω–Ω–æ_–ø—Ä–∞–≤–æ–≤–∞—è_—Ñ–æ—Ä–º–∞'] = cls.clean_value(legal_form)
        
        result['–ø–æ–ª–Ω–æ–µ_–Ω–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ'] = cls.clean_value(
            cls.safe_get(company_data, 'fullName') or
            cls.safe_get(company_data, 'legalName')
        )
        
        result['—Å–æ–∫—Ä–∞—â–µ–Ω–Ω–æ–µ_–Ω–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ_–∑–∞—è–≤–∏—Ç–µ–ª—è'] = cls.clean_value(
            cls.safe_get(company_data, 'shortName') or
            cls.safe_get(company_data, 'abbreviation')
        )
        
        result['–≥–æ—Å—É–¥–∞—Ä—Å—Ç–≤–µ–Ω–Ω–æ–µ_–ø—Ä–µ–¥–ø—Ä–∏—è—Ç–∏–µ'] = cls.clean_value(
            cls.safe_get(company_data, 'isStateOwned', False)
        )
        
        result['–∏–Ω–æ—Å—Ç—Ä–∞–Ω–Ω–∞—è_–æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏—è'] = cls.clean_value(
            cls.safe_get(company_data, 'isForeign', False)
        )
        
        # –†–µ–∫–≤–∏–∑–∏—Ç—ã
        result['–∏–Ω–Ω'] = cls.clean_value(cls.safe_get(company_data, 'inn'))
        result['–∫–ø–ø'] = cls.clean_value(cls.safe_get(company_data, 'kpp'))
        result['–æ–≥—Ä–Ω'] = cls.clean_value(cls.safe_get(company_data, 'ogrn'))
        result['–æ–∫–ø–æ'] = cls.clean_value(cls.safe_get(company_data, 'okpo'))
        result['–æ–∫–æ–≥—É'] = cls.clean_value(cls.safe_get(company_data, 'okogu'))
        result['–æ–∫—Ñ—Å'] = cls.clean_value(cls.safe_get(company_data, 'okfs'))
        
        # –ê–¥—Ä–µ—Å–∞
        result['–∞–¥—Ä–µ—Å_–º–µ—Å—Ç–∞_–Ω–∞—Ö–æ–∂–¥–µ–Ω–∏—è'] = cls.clean_value(
            cls.safe_get(company_data, 'legalAddress.fullAddress') or
            cls.safe_get(company_data, 'legalAddress')
        )
        
        result['–∞–¥—Ä–µ—Å_–ø–æ—á—Ç–æ–≤—ã–π'] = cls.clean_value(
            cls.safe_get(company_data, 'postalAddress')
        )
        
        # –ù–∞–ª–æ–≥–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ
        result['–Ω–∞–ª–æ–≥–æ–≤—ã–π_–æ—Ä–≥–∞–Ω'] = cls.clean_value(
            cls.safe_get(company_data, 'taxAuthority.name') or
            cls.safe_get(company_data, 'taxAuthority')
        )
        
        result['–¥–∞—Ç–∞_–ø–æ—Å—Ç–∞–Ω–æ–≤–∫–∏_–Ω–∞_—É—á–µ—Ç'] = cls.clean_value(
            cls.safe_get(company_data, 'registrationDate')
        )
        
        # –†—É–∫–æ–≤–æ–¥–∏—Ç–µ–ª—å –∑–∞—è–≤–∏—Ç–µ–ª—è
        result['—Ñ–∏–æ_—Ä—É–∫–æ–≤–æ–¥–∏—Ç–µ–ª—è_–∑–∞—è–≤–∏—Ç–µ–ª—è'] = cls.clean_value(
            cls.safe_get(company_data, 'director.fullName') or
            cls.safe_get(company_data, 'head.fullName') or
            cls.safe_get(company_data, 'generalDirector')
        )
        
        result['–¥–æ–ª–∂–Ω–æ—Å—Ç—å_—Ä—É–∫–æ–≤–æ–¥–∏—Ç–µ–ª—è_–∑–∞—è–≤–∏—Ç–µ–ª—è'] = cls.clean_value(
            cls.safe_get(company_data, 'director.position')
        )
        
        result['—Ç–µ–ª–µ—Ñ–æ–Ω_–∑–∞—è–≤–∏—Ç–µ–ª—è'] = cls.clean_value(
            cls.safe_get(company_data, 'contactPhone') or
            cls.safe_get(company_data, 'phone')
        )
        
        result['email_–∑–∞—è–≤–∏—Ç–µ–ª—è'] = cls.clean_value(
            cls.safe_get(company_data, 'contactEmail') or
            cls.safe_get(company_data, 'email')
        )
        
        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø–æ–ª—è –∏–∑ PDF
        result['–Ω–æ–º–µ—Ä_–ø–æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è_–∞–∫—Ç—É–∞–ª–∏–∑–∞—Ü–∏–∏'] = cls.clean_value(
            cls.safe_get(company_data, 'updateDecreeNumber')
        )
        
        result['–¥–∞—Ç–∞_–ø–æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è_–∞–∫—Ç—É–∞–ª–∏–∑–∞—Ü–∏–∏'] = cls.clean_value(
            cls.safe_get(company_data, 'updateDecreeDate')
        )
        
        return result
    
    @classmethod
    def extract_additional_data(cls, company_data: Dict, decl_data: Dict) -> Dict[str, Any]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö"""
        result = {}
        
        # –ö–æ–¥—ã –¥–µ—è—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
        result['–æ–∫–≤—ç–¥'] = cls.clean_value(cls.safe_get(company_data, 'okved'))
        result['–æ–∫–ø–¥2'] = cls.clean_value(cls.safe_get(company_data, 'okpd2'))
        
        # –°–≤–∏–¥–µ—Ç–µ–ª—å—Å—Ç–≤–∞
        result['–Ω–æ–º–µ—Ä_—Å–≤–∏–¥–µ—Ç–µ–ª—å—Å—Ç–≤–∞'] = cls.clean_value(
            cls.safe_get(company_data, 'certificateNumber')
        )
        
        result['–¥–∞—Ç–∞_–≤—ã–¥–∞—á–∏_—Å–≤–∏–¥–µ—Ç–µ–ª—å—Å—Ç–≤–∞'] = cls.clean_value(
            cls.safe_get(company_data, 'certificateIssueDate')
        )
        
        result['—Å—Ä–æ–∫_–¥–µ–π—Å—Ç–≤–∏—è_—Å–≤–∏–¥–µ—Ç–µ–ª—å—Å—Ç–≤–∞'] = cls.clean_value(
            cls.safe_get(company_data, 'certificateValidUntil')
        )
        
        # –õ–∞–±–æ—Ä–∞—Ç–æ—Ä–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
        result['—Ç–∏–ø_–ª–∞–±–æ—Ä–∞—Ç–æ—Ä–∏–∏'] = cls.clean_value(
            cls.safe_get(decl_data, 'laboratoryType')
        )
        
        result['–æ–±–ª–∞—Å—Ç—å_–∞–∫–∫—Ä–µ–¥–∏—Ç–∞—Ü–∏–∏_–∫–æ–¥—ã'] = cls.clean_value(
            cls.safe_get(decl_data, 'accreditationCodes')
        )
        
        # –§–∞–π–ª—ã –∏ –¥–æ–∫—É–º–µ–Ω—Ç—ã
        files = cls.safe_get(decl_data, 'files', [])
        if isinstance(files, list):
            file_list = []
            for file_item in files:
                if isinstance(file_item, dict):
                    name = file_item.get('name', '')
                    url = file_item.get('url', '')
                    if url:
                        if not url.startswith('http'):
                            url = f"https://pub.fsa.gov.ru{url}"
                        file_list.append(f"{name}: {url}")
            result['–ø—Ä–∏–∫—Ä–µ–ø–ª–µ–Ω–Ω—ã–µ_—Ñ–∞–π–ª—ã'] = cls.clean_value(" | ".join(file_list))
        
        return result
    
    @classmethod
    def process_company(cls, company_id: str, company_data: Dict, decl_data: Dict) -> Dict[str, Any]:
        """–ü–æ–ª–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∫–æ–º–ø–∞–Ω–∏–∏"""
        result = {
            'id_–∫–æ–º–ø–∞–Ω–∏–∏': company_id,
            '–∏—Å—Ç–æ—á–Ω–∏–∫_–¥–∞–Ω–Ω—ã—Ö': 'https://pub.fsa.gov.ru/ral',
            '–¥–∞—Ç–∞_–ø–∞—Ä—Å–∏–Ω–≥–∞': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
            '—Å—Ç–∞—Ç—É—Å_–ø–∞—Ä—Å–∏–Ω–≥–∞': '–£–°–ü–ï–®–ù–û'
        }
        
        try:
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –¥–∞–Ω–Ω—ã–µ –∏–∑ –≤—Å–µ—Ö —Ä–∞–∑–¥–µ–ª–æ–≤
            accredited_data = cls.extract_accredited_person(company_data, decl_data)
            applicant_data = cls.extract_applicant_data(company_data)
            additional_data = cls.extract_additional_data(company_data, decl_data)
            
            # –û–±—ä–µ–¥–∏–Ω—è–µ–º –≤—Å–µ –¥–∞–Ω–Ω—ã–µ
            result.update(accredited_data)
            result.update(applicant_data)
            result.update(additional_data)
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –∞–∫–∫—Ä–µ–¥–∏—Ç–∞—Ü–∏–∏
            if not result.get('—É–Ω–∏–∫–∞–ª—å–Ω—ã–π_–Ω–æ–º–µ—Ä_–∑–∞–ø–∏—Å–∏'):
                result['—Å—Ç–∞—Ç—É—Å'] = '–ù–µ—Ç –¥–µ–π—Å—Ç–≤—É—é—â–µ–π –∞–∫–∫—Ä–µ–¥–∏—Ç–∞—Ü–∏–∏'
                result['—Å—Ç–∞—Ç—É—Å_–ø–∞—Ä—Å–∏–Ω–≥–∞'] = '–ù–ï–¢_–ê–ö–ö–†–ï–î–ò–¢–ê–¶–ò–ò'
            
            logger.debug(f"–û–±—Ä–∞–±–æ—Ç–∞–Ω–∞ –∫–æ–º–ø–∞–Ω–∏—è {company_id}: {result.get('–Ω–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ', '–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è')}")
            
        except Exception as e:
            result['—Å—Ç–∞—Ç—É—Å_–ø–∞—Ä—Å–∏–Ω–≥–∞'] = f'–û–®–ò–ë–ö–ê: {str(e)[:100]}'
            logger.error(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∫–æ–º–ø–∞–Ω–∏–∏ {company_id}: {e}")
        
        return result

# ================= –ö–õ–ê–°–° –î–õ–Ø –£–ü–†–ê–í–õ–ï–ù–ò–Ø –î–ê–ù–ù–´–ú–ò =================
class DataManager:
    """–£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –∑–∞–≥—Ä—É–∑–∫–æ–π –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º –¥–∞–Ω–Ω—ã—Ö"""
    
    @staticmethod
    def extract_company_id(source: str) -> str:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ ID –∫–æ–º–ø–∞–Ω–∏–∏ –∏–∑ URL –∏–ª–∏ —Å—Ç—Ä–æ–∫–∏"""
        if not source:
            return ""
        
        if 'pub.fsa.gov.ru' in source:
            # –û–±—Ä–∞–±–æ—Ç–∫–∞ URL
            parts = source.strip('/').split('/')
            for i, part in enumerate(parts):
                if part == 'view' and i + 1 < len(parts):
                    return parts[i + 1]
            # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ —á–µ—Ä–µ–∑ view, –±–µ—Ä–µ–º –ø—Ä–µ–¥–ø–æ—Å–ª–µ–¥–Ω—é—é —á–∞—Å—Ç—å
            return parts[-2] if len(parts) > 2 else ""
        
        # –ï—Å–ª–∏ —ç—Ç–æ —É–∂–µ ID
        return str(source).strip()
    
    @classmethod
    def load_company_ids(cls) -> List[str]:
        """–ó–∞–≥—Ä—É–∑–∫–∞ —Å–ø–∏—Å–∫–∞ ID –∫–æ–º–ø–∞–Ω–∏–π –∏–∑ —Ñ–∞–π–ª–æ–≤"""
        possible_files = [
            "company_ids.txt",   # –¢–æ–ª—å–∫–æ ID
            "links.txt",         # –°—Å—ã–ª–∫–∏
            "ids.txt",           # –ü—Ä–æ—Å—Ç–æ–π —Å–ø–∏—Å–æ–∫
            "input.txt",         # –û–±—â–∏–π —Ñ–∞–π–ª
            "—Å–ø–∏—Å–æ–∫.txt",        # –†—É—Å—Å–∫–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ
        ]
        
        all_ids = []
        
        for filename in possible_files:
            filepath = Path(filename)
            if filepath.exists():
                try:
                    logger.info(f"–ó–∞–≥—Ä—É–∑–∫–∞ ID –∏–∑ {filename}")
                    
                    content = filepath.read_text(encoding='utf-8').strip()
                    lines = [line.strip() for line in content.split('\n') if line.strip()]
                    
                    for line in lines:
                        # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏
                        if line.startswith('#') or line.startswith('//'):
                            continue
                        
                        company_id = cls.extract_company_id(line)
                        if company_id and company_id.isdigit():
                            all_ids.append(company_id)
                        elif line.isdigit():
                            all_ids.append(line)
                            
                    logger.info(f"  –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(lines)} —Å—Ç—Ä–æ–∫ –∏–∑ {filename}")
                    
                except Exception as e:
                    logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ {filename}: {e}")
        
        # –£–¥–∞–ª—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã, —Å–æ—Ö—Ä–∞–Ω—è—è –ø–æ—Ä—è–¥–æ–∫
        seen = set()
        unique_ids = []
        for id_ in all_ids:
            if id_ not in seen:
                seen.add(id_)
                unique_ids.append(id_)
        
        logger.info(f"–í—Å–µ–≥–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö ID: {len(unique_ids)}")
        
        # –ï—Å–ª–∏ –Ω–µ—Ç —Ñ–∞–π–ª–æ–≤, –≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Ç–µ—Å—Ç–æ–≤—ã–µ ID
        if not unique_ids:
            logger.warning("–§–∞–π–ª—ã —Å ID –Ω–µ –Ω–∞–π–¥–µ–Ω—ã. –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ—Å—Ç–æ–≤—ã—Ö ID 1-1000")
            unique_ids = [str(i) for i in range(1, 1001)]
        
        # –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –∑–∞–ø–∏—Å–µ–π
        if CONFIG["max_records"] > 0:
            unique_ids = unique_ids[:CONFIG["max_records"]]
            logger.info(f"–û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –¥–æ {CONFIG['max_records']} –∑–∞–ø–∏—Å–µ–π")
        
        return unique_ids
    
    @staticmethod
    def save_results(data: List[Dict], filename: str, format_type: str = 'excel'):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤ —Ñ–∞–π–ª"""
        try:
            if not data:
                logger.warning("–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è")
                return False
            
            df = pd.DataFrame(data)
            
            # –£–ø–æ—Ä—è–¥–æ—á–∏–≤–∞–µ–º —Å—Ç–æ–ª–±—Ü—ã
            priority_columns = [
                'id_–∫–æ–º–ø–∞–Ω–∏–∏', '—Å—Ç–∞—Ç—É—Å', '—É–Ω–∏–∫–∞–ª—å–Ω—ã–π_–Ω–æ–º–µ—Ä_–∑–∞–ø–∏—Å–∏',
                '–Ω–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ', '—Å–æ–∫—Ä–∞—â–µ–Ω–Ω–æ–µ_–Ω–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ', '–∏–Ω–Ω',
                '—Ç–∏–ø_–∑–∞—è–≤–∏—Ç–µ–ª—è', '–æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–æ–Ω–Ω–æ_–ø—Ä–∞–≤–æ–≤–∞—è_—Ñ–æ—Ä–º–∞',
                '–¥–∞—Ç–∞_–≤–Ω–µ—Å–µ–Ω–∏—è_–≤_—Ä–µ–µ—Å—Ç—Ä', '–¥–∞—Ç–∞_–ø–∞—Ä—Å–∏–Ω–≥–∞'
            ]
            
            existing_columns = list(df.columns)
            ordered_columns = []
            
            # –°–Ω–∞—á–∞–ª–∞ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–Ω—ã–µ
            for col in priority_columns:
                if col in existing_columns:
                    ordered_columns.append(col)
                    existing_columns.remove(col)
            
            # –ó–∞—Ç–µ–º –æ—Å—Ç–∞–ª—å–Ω—ã–µ –≤ –∞–ª—Ñ–∞–≤–∏—Ç–Ω–æ–º –ø–æ—Ä—è–¥–∫–µ
            ordered_columns.extend(sorted(existing_columns))
            df = df[ordered_columns]
            
            if format_type == 'excel':
                # Excel
                excel_file = filename if filename.endswith('.xlsx') else f"{filename}.xlsx"
                df.to_excel(excel_file, index=False, engine='openpyxl')
                logger.info(f"–°–æ—Ö—Ä–∞–Ω–µ–Ω–æ {len(df)} –∑–∞–ø–∏—Å–µ–π –≤ Excel: {excel_file}")
                
                # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ —Å–æ—Ö—Ä–∞–Ω—è–µ–º –≤ CSV
                csv_file = excel_file.replace('.xlsx', '.csv')
                df.to_csv(csv_file, index=False, encoding='utf-8-sig')
                logger.info(f"–†–µ–∑–µ—Ä–≤–Ω–∞—è –∫–æ–ø–∏—è –≤ CSV: {csv_file}")
                
            elif format_type == 'csv':
                # –¢–æ–ª—å–∫–æ CSV
                csv_file = filename if filename.endswith('.csv') else f"{filename}.csv"
                df.to_csv(csv_file, index=False, encoding='utf-8-sig')
                logger.info(f"–°–æ—Ö—Ä–∞–Ω–µ–Ω–æ {len(df)} –∑–∞–ø–∏—Å–µ–π –≤ CSV: {csv_file}")
            
            elif format_type == 'json':
                # JSON
                json_file = filename if filename.endswith('.json') else f"{filename}.json"
                with open(json_file, 'w', encoding='utf-8') as f:
                    json.dump(data, f, ensure_ascii=False, indent=2)
                logger.info(f"–°–æ—Ö—Ä–∞–Ω–µ–Ω–æ {len(data)} –∑–∞–ø–∏—Å–µ–π –≤ JSON: {json_file}")
            
            return True
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ñ–∞–π–ª–∞ {filename}: {e}")
            logger.error(traceback.format_exc())
            return False

# ================= –ö–õ–ê–°–° –î–õ–Ø –ü–û–ò–°–ö–ê ID –í –†–ï–ï–°–¢–†–ï =================
class IDFinder:
    """–ü–æ–∏—Å–∫ ID –∫–æ–º–ø–∞–Ω–∏–π –≤ —Ä–µ–µ—Å—Ç—Ä–µ"""
    
    @staticmethod
    def find_ids_from_api() -> List[str]:
        """–ü–æ–∏—Å–∫ ID —á–µ—Ä–µ–∑ API (–µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω–æ)"""
        logger.info("–ü–æ–ø—ã—Ç–∫–∞ –Ω–∞–π—Ç–∏ ID —á–µ—Ä–µ–∑ API...")
        
        try:
            # –≠—Ç–æ—Ç URL –º–æ–∂–µ—Ç –ø–æ—Ç—Ä–µ–±–æ–≤–∞—Ç—å –Ω–∞—Å—Ç—Ä–æ–π–∫–∏
            search_url = "https://pub.fsa.gov.ru/api/v1/ral/common/search"
            
            headers = {
                "authorization": CONFIG["auth_token"],
                "user-agent": CONFIG["user_agent"],
            }
            
            # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –ø–æ–∏—Å–∫–∞ (–º–æ–∂–µ—Ç –ø–æ—Ç—Ä–µ–±–æ–≤–∞—Ç—å—Å—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∞)
            params = {
                "page": 0,
                "size": 100,  # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ
                "sort": "id,asc"
            }
            
            response = requests.get(search_url, headers=headers, params=params, timeout=30)
            
            if response.status_code == 200:
                data = response.json()
                ids = []
                
                # –ò–∑–≤–ª–µ–∫–∞–µ–º ID –∏–∑ –æ—Ç–≤–µ—Ç–∞ (—Å—Ç—Ä—É–∫—Ç—É—Ä–∞ –º–æ–∂–µ—Ç –æ—Ç–ª–∏—á–∞—Ç—å—Å—è)
                if isinstance(data, dict):
                    content = data.get('content', [])
                    if isinstance(content, list):
                        for item in content:
                            if isinstance(item, dict):
                                company_id = item.get('id')
                                if company_id:
                                    ids.append(str(company_id))
                
                logger.info(f"–ù–∞–π–¥–µ–Ω–æ {len(ids)} ID —á–µ—Ä–µ–∑ API")
                return ids
            else:
                logger.warning(f"API –ø–æ–∏—Å–∫–∞ –≤–µ—Ä–Ω—É–ª {response.status_code}")
                
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ —á–µ—Ä–µ–∑ API: {e}")
        
        return []
    
    @staticmethod
    def generate_id_range(start: int = 1, end: int = 38000) -> List[str]:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –¥–∏–∞–ø–∞–∑–æ–Ω–∞ ID"""
        logger.info(f"–ì–µ–Ω–µ—Ä–∞—Ü–∏—è ID —Å {start} –ø–æ {end}")
        return [str(i) for i in range(start, end + 1)]

# ================= –û–°–ù–û–í–ù–û–ô –ö–õ–ê–°–° –ü–ê–†–°–ï–†–ê =================
class FSAParser:
    """–û—Å–Ω–æ–≤–Ω–æ–π –∫–ª–∞—Å—Å –ø–∞—Ä—Å–µ—Ä–∞"""
    
    def __init__(self):
        self.data_manager = DataManager()
        self.data_extractor = DataExtractor()
        self.id_finder = IDFinder()
        self.processed_count = 0
        self.failed_count = 0
        self.start_time = None
        
    async def process_single_company(self, api_client: FSAApiClient, company_id: str) -> Optional[Dict]:
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ –æ–¥–Ω–æ–π –∫–æ–º–ø–∞–Ω–∏–∏"""
        try:
            logger.debug(f"–ó–∞–ø—Ä–æ—Å –¥–∞–Ω–Ω—ã—Ö –∫–æ–º–ø–∞–Ω–∏–∏ {company_id}")
            
            # –ü–æ–ª—É—á–∞–µ–º –¥–∞–Ω–Ω—ã–µ –∫–æ–º–ø–∞–Ω–∏–∏
            company_data = await api_client.get_company_data(company_id)
            
            if not company_data or company_data.get('_status') in ['NOT_FOUND', 'FAILED']:
                logger.warning(f"–ö–æ–º–ø–∞–Ω–∏—è {company_id} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")
                return {
                    'id_–∫–æ–º–ø–∞–Ω–∏–∏': company_id,
                    '—Å—Ç–∞—Ç—É—Å': '–ù–µ –Ω–∞–π–¥–µ–Ω–æ –≤ —Ä–µ–µ—Å—Ç—Ä–µ',
                    '–¥–∞—Ç–∞_–ø–∞—Ä—Å–∏–Ω–≥–∞': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                    '—Å—Ç–∞—Ç—É—Å_–ø–∞—Ä—Å–∏–Ω–≥–∞': '–ù–ï_–ù–ê–ô–î–ï–ù–û'
                }
            
            # –ò—â–µ–º –¥–∞–Ω–Ω—ã–µ –¥–µ–∫–ª–∞—Ä–∞—Ü–∏–∏
            decl_data = {"_status": "NO_DATA"}
            accreditation = DataExtractor.safe_get(company_data, 'accreditation')
            
            if isinstance(accreditation, dict):
                doc_id = accreditation.get('idAccredScopeFile') or accreditation.get('id')
                
                if doc_id:
                    logger.debug(f"–ó–∞–ø—Ä–æ—Å –¥–µ–∫–ª–∞—Ä–∞—Ü–∏–∏ {doc_id}")
                    decl_data = await api_client.get_declaration_data(doc_id)
                    
                    if decl_data.get('_status') == 'NOT_FOUND':
                        logger.debug(f"–î–µ–∫–ª–∞—Ä–∞—Ü–∏—è –¥–ª—è {company_id} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")
                    else:
                        logger.debug(f"–ü–æ–ª—É—á–µ–Ω–∞ –¥–µ–∫–ª–∞—Ä–∞—Ü–∏—è –¥–ª—è {company_id}")
            
            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ
            result = self.data_extractor.process_company(company_id, company_data, decl_data)
            
            self.processed_count += 1
            return result
            
        except Exception as e:
            self.failed_count += 1
            logger.error(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∫–æ–º–ø–∞–Ω–∏–∏ {company_id}: {e}")
            logger.error(traceback.format_exc())
            
            return {
                'id_–∫–æ–º–ø–∞–Ω–∏–∏': company_id,
                '—Å—Ç–∞—Ç—É—Å': f'–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏: {str(e)[:100]}',
                '–¥–∞—Ç–∞_–ø–∞—Ä—Å–∏–Ω–≥–∞': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                '—Å—Ç–∞—Ç—É—Å_–ø–∞—Ä—Å–∏–Ω–≥–∞': '–û–®–ò–ë–ö–ê'
            }
    
    async def run(self):
        """–û—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥ –∑–∞–ø—É—Å–∫–∞ –ø–∞—Ä—Å–µ—Ä–∞"""
        logger.info("=" * 70)
        logger.info("üöÄ –ó–ê–ü–£–°–ö –ü–ê–†–°–ï–†–ê –†–ï–ï–°–¢–†–ê –§–°–ê")
        logger.info("=" * 70)
        
        self.start_time = time.time()
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º ID –∫–æ–º–ø–∞–Ω–∏–π
        company_ids = self.data_manager.load_company_ids()
        
        if not company_ids:
            logger.error("‚ùå –ù–µ—Ç ID –∫–æ–º–ø–∞–Ω–∏–π –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏!")
            return
        
        total = len(company_ids)
        logger.info(f"üìä –í—Å–µ–≥–æ –∑–∞–ø–∏—Å–µ–π –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏: {total}")
        
        # –°–æ–∑–¥–∞–µ–º —Å–ø–∏—Å–æ–∫ –¥–ª—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        all_results = []
        batch_results = []
        
        # –°–æ–∑–¥–∞–µ–º —Å–µ–º–∞—Ñ–æ—Ä –¥–ª—è –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
        sem = asyncio.Semaphore(CONFIG["concurrency"])
        
        async def process_with_limit(company_id: str, idx: int):
            """–û–±—Ä–∞–±–æ—Ç–∫–∞ —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ–º –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤"""
            async with sem:
                # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º, –µ—Å–ª–∏ –¥–æ—Å—Ç–∏–≥–ª–∏ –ª–∏–º–∏—Ç–∞
                if CONFIG["max_records"] > 0 and idx > CONFIG["max_records"]:
                    return None
                
                return await self.process_single_company(api_client, company_id)
        
        # –°–æ–∑–¥–∞–µ–º API –∫–ª–∏–µ–Ω—Ç
        async with FSAApiClient() as api_client:
            # –°–æ–∑–¥–∞–µ–º –∑–∞–¥–∞—á–∏
            tasks = []
            for idx, company_id in enumerate(company_ids, 1):
                task = process_with_limit(company_id, idx)
                tasks.append(task)
            
            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∑–∞–¥–∞—á–∏ –ø–æ –º–µ—Ä–µ –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç–∏
            current_batch = 0
            
            for idx, task in enumerate(asyncio.as_completed(tasks), 1):
                try:
                    result = await task
                    
                    if result:
                        batch_results.append(result)
                        all_results.append(result)
                        
                        # –ü—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω–æ–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
                        if len(batch_results) >= CONFIG["batch_size"]:
                            self.data_manager.save_results(
                                batch_results, 
                                f"—á–∞—Å—Ç—å_{current_batch + 1}_{CONFIG['output_excel']}",
                                'excel'
                            )
                            
                            # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–∞–∫–∂–µ –≤ –æ–±—â–∏–π —Ñ–∞–π–ª
                            self.data_manager.save_results(
                                all_results,
                                CONFIG["output_excel"],
                                'excel'
                            )
                            
                            batch_results = []
                            current_batch += 1
                    
                    # –í—ã–≤–æ–¥ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞
                    if idx % 100 == 0 or idx == total:
                        elapsed = time.time() - self.start_time
                        processed = self.processed_count + self.failed_count
                        
                        if elapsed > 0:
                            speed = processed / elapsed
                            remaining = (total - processed) / speed if speed > 0 else 0
                            
                            logger.info(
                                f"üìà –ü—Ä–æ–≥—Ä–µ—Å—Å: {processed}/{total} ({processed/total*100:.1f}%) | "
                                f"–°–∫–æ—Ä–æ—Å—Ç—å: {speed:.1f}/—Å–µ–∫ | "
                                f"–û—à–∏–±–æ–∫: {self.failed_count} | "
                                f"–û—Å—Ç–∞–ª–æ—Å—å: ~{remaining/60:.0f} –º–∏–Ω"
                            )
                
                except Exception as e:
                    logger.error(f"–û—à–∏–±–∫–∞ –≤ –æ—Å–Ω–æ–≤–Ω–æ–π –∑–∞–¥–∞—á–µ {idx}: {e}")
                    self.failed_count += 1
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Å—Ç–∞–≤—à–∏–µ—Å—è –¥–∞–Ω–Ω—ã–µ
        if batch_results:
            self.data_manager.save_results(
                batch_results,
                f"—á–∞—Å—Ç—å_{current_batch + 1}_{CONFIG['output_excel']}",
                'excel'
            )
        
        # –§–∏–Ω–∞–ª—å–Ω–æ–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
        self.data_manager.save_results(all_results, CONFIG["output_excel"], 'excel')
        self.data_manager.save_results(all_results, CONFIG["output_csv"], 'csv')
        self.data_manager.save_results(all_results, CONFIG["output_json"], 'json')
        
        # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç—á–µ—Ç–∞
        self.generate_report(total, all_results)
    
    def generate_report(self, total: int, results: List[Dict]):
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∏—Ç–æ–≥–æ–≤–æ–≥–æ –æ—Ç—á–µ—Ç–∞"""
        end_time = time.time()
        total_time = end_time - self.start_time
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —Å—Ç–∞—Ç—É—Å–∞–º
        status_stats = {}
        for result in results:
            status = result.get('—Å—Ç–∞—Ç—É—Å', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ')
            status_stats[status] = status_stats.get(status, 0) + 1
        
        logger.info("=" * 70)
        logger.info("‚úÖ –ü–ê–†–°–ò–ù–ì –ó–ê–í–ï–†–®–ï–ù!")
        logger.info("=" * 70)
        logger.info(f"üìä –û–ë–©–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê:")
        logger.info(f"   –í—Å–µ–≥–æ –∑–∞–ø–∏—Å–µ–π: {total}")
        logger.info(f"   –£—Å–ø–µ—à–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {self.processed_count}")
        logger.info(f"   –ó–∞–ø–∏—Å–µ–π —Å –æ—à–∏–±–∫–∞–º–∏: {self.failed_count}")
        logger.info(f"   –ü—Ä–æ—Ü–µ–Ω—Ç —É—Å–ø–µ—Ö–∞: {(self.processed_count/(self.processed_count + self.failed_count))*100:.1f}%")
        logger.info(f"‚è±Ô∏è  –í–†–ï–ú–Ø –í–´–ü–û–õ–ù–ï–ù–ò–Ø:")
        logger.info(f"   –û–±—â–µ–µ –≤—Ä–µ–º—è: {total_time/60:.1f} –º–∏–Ω—É—Ç")
        logger.info(f"   –°—Ä–µ–¥–Ω—è—è —Å–∫–æ—Ä–æ—Å—Ç—å: {self.processed_count/total_time:.2f} –∑–∞–ø–∏—Å–µ–π/—Å–µ–∫")
        logger.info(f"üíæ –í–´–•–û–î–ù–´–ï –§–ê–ô–õ–´:")
        logger.info(f"   –û—Å–Ω–æ–≤–Ω–æ–π Excel: {CONFIG['output_excel']}")
        logger.info(f"   –†–µ–∑–µ—Ä–≤–Ω—ã–π CSV: {CONFIG['output_csv']}")
        logger.info(f"   –†–µ–∑–µ—Ä–≤–Ω—ã–π JSON: {CONFIG['output_json']}")
        logger.info(f"   –õ–æ–≥ —Ñ–∞–π–ª: {CONFIG['log_file']}")
        logger.info(f"üìà –°–¢–ê–¢–ò–°–¢–ò–ö–ê –ü–û –°–¢–ê–¢–£–°–ê–ú:")
        for status, count in sorted(status_stats.items(), key=lambda x: x[1], reverse=True)[:10]:
            logger.info(f"   {status}: {count}")
        logger.info("=" * 70)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Ç—á–µ—Ç –≤ —Ñ–∞–π–ª
        report = f"""
–û–¢–ß–ï–¢ –û –ü–ê–†–°–ò–ù–ì–ï –†–ï–ï–°–¢–†–ê –§–°–ê
{'=' * 50}

–î–ê–¢–ê –í–´–ü–û–õ–ù–ï–ù–ò–Ø: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

–°–¢–ê–¢–ò–°–¢–ò–ö–ê:
- –í—Å–µ–≥–æ –∑–∞–ø–∏—Å–µ–π –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏: {total}
- –£—Å–ø–µ—à–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {self.processed_count}
- –ó–∞–ø–∏—Å–µ–π —Å –æ—à–∏–±–∫–∞–º–∏: {self.failed_count}
- –ü—Ä–æ—Ü–µ–Ω—Ç —É—Å–ø–µ—Ö–∞: {(self.processed_count/(self.processed_count + self.failed_count))*100:.1f}%

–í–†–ï–ú–Ø –í–´–ü–û–õ–ù–ï–ù–ò–Ø:
- –û–±—â–µ–µ –≤—Ä–µ–º—è: {total_time/60:.1f} –º–∏–Ω—É—Ç
- –°—Ä–µ–¥–Ω—è—è —Å–∫–æ—Ä–æ—Å—Ç—å: {self.processed_count/total_time:.2f} –∑–∞–ø–∏—Å–µ–π/—Å–µ–∫

–í–´–•–û–î–ù–´–ï –§–ê–ô–õ–´:
1. {CONFIG['output_excel']} - –æ—Å–Ω–æ–≤–Ω–æ–π —Ñ–∞–π–ª Excel
2. {CONFIG['output_csv']} - —Ä–µ–∑–µ—Ä–≤–Ω—ã–π —Ñ–∞–π–ª CSV
3. {CONFIG['output_json']} - —Ä–µ–∑–µ—Ä–≤–Ω—ã–π —Ñ–∞–π–ª JSON
4. {CONFIG['log_file']} - —Ñ–∞–π–ª –ª–æ–≥–æ–≤

–†–ê–°–ü–†–ï–î–ï–õ–ï–ù–ò–ï –ü–û –°–¢–ê–¢–£–°–ê–ú:
"""
        
        for status, count in sorted(status_stats.items(), key=lambda x: x[1], reverse=True):
            report += f"- {status}: {count}\n"
        
        report_path = "–æ—Ç—á–µ—Ç_–æ_–ø–∞—Ä—Å–∏–Ω–≥–µ.txt"
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write(report)
        
        logger.info(f"üìÑ –û—Ç—á–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤: {report_path}")

# ================= –í–°–ü–û–ú–û–ì–ê–¢–ï–õ–¨–ù–´–ï –§–£–ù–ö–¶–ò–ò =================
def check_environment():
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ –æ–∫—Ä—É–∂–µ–Ω–∏—è –∏ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π"""
    print("üîç –ü–†–û–í–ï–†–ö–ê –û–ö–†–£–ñ–ï–ù–ò–Ø...")
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ Python
    print(f"  Python: {sys.version.split()[0]}")
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –±–∏–±–ª–∏–æ—Ç–µ–∫
    required = ['aiohttp', 'pandas', 'openpyxl']
    for lib in required:
        try:
            __import__(lib)
            print(f"  ‚úÖ {lib}")
        except ImportError:
            print(f"  ‚ùå {lib} - —Ç—Ä–µ–±—É–µ—Ç—Å—è —É—Å—Ç–∞–Ω–æ–≤–∫–∞")
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ API
    print("\nüîç –ü–†–û–í–ï–†–ö–ê –î–û–°–¢–£–ü–ù–û–°–¢–ò API...")
    
    test_url = "https://pub.fsa.gov.ru/api/v1/ral/common/companies/1"
    headers = {
        "authorization": CONFIG["auth_token"],
        "user-agent": CONFIG["user_agent"],
    }
    
    try:
        response = requests.get(test_url, headers=headers, timeout=10)
        print(f"  –°—Ç–∞—Ç—É—Å API: {response.status_code}")
        
        if response.status_code == 200:
            print("  ‚úÖ API –¥–æ—Å—Ç—É–ø–µ–Ω")
            data = response.json()
            print(f"  –ü—Ä–∏–º–µ—Ä –¥–∞–Ω–Ω—ã—Ö:")
            print(f"    ID: {data.get('id')}")
            print(f"    –ù–∞–∑–≤–∞–Ω–∏–µ: {data.get('fullName')}")
            print(f"    –ò–ù–ù: {data.get('inn')}")
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø—Ä–∏–º–µ—Ä
            with open("–ø—Ä–∏–º–µ—Ä_api.json", "w", encoding="utf-8") as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
            print("  –ü—Ä–∏–º–µ—Ä —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ '–ø—Ä–∏–º–µ—Ä_api.json'")
            
        elif response.status_code == 401:
            print("  ‚ùå –û—à–∏–±–∫–∞ –∞–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ —Ç–æ–∫–µ–Ω –≤ CONFIG['auth_token']")
        else:
            print(f"  ‚ö†Ô∏è API –≤–µ—Ä–Ω—É–ª –∫–æ–¥: {response.status_code}")
            
    except Exception as e:
        print(f"  ‚ùå –û—à–∏–±–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è: {e}")

def create_sample_files():
    """–°–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–∏–º–µ—Ä–æ–≤ —Ñ–∞–π–ª–æ–≤"""
    print("\nüìÅ –°–û–ó–î–ê–ù–ò–ï –ü–†–ò–ú–ï–†–ù–´–• –§–ê–ô–õ–û–í...")
    
    # 1. –§–∞–π–ª —Å ID
    sample_ids = [str(i) for i in range(1, 101)]  # 1-100
    with open("–ø—Ä–∏–º–µ—Ä_company_ids.txt", "w", encoding="utf-8") as f:
        f.write("# –ü—Ä–∏–º–µ—Ä —Ñ–∞–π–ª–∞ —Å ID –∫–æ–º–ø–∞–Ω–∏–π\n")
        f.write("# –ö–∞–∂–¥–∞—è —Å—Ç—Ä–æ–∫–∞ - –æ–¥–∏–Ω ID\n\n")
        f.write("\n".join(sample_ids))
    print("  –°–æ–∑–¥–∞–Ω: –ø—Ä–∏–º–µ—Ä_company_ids.txt (100 ID)")
    
    # 2. –§–∞–π–ª —Å–æ —Å—Å—ã–ª–∫–∞–º–∏
    sample_links = [
        "https://pub.fsa.gov.ru/ral/view/1/current-aa",
        "https://pub.fsa.gov.ru/ral/view/2/current-aa",
        "https://pub.fsa.gov.ru/ral/view/3/current-aa",
    ]
    with open("–ø—Ä–∏–º–µ—Ä_links.txt", "w", encoding="utf-8") as f:
        f.write("# –ü—Ä–∏–º–µ—Ä —Ñ–∞–π–ª–∞ —Å–æ —Å—Å—ã–ª–∫–∞–º–∏\n")
        f.write("# –ü–∞—Ä—Å–µ—Ä –∏–∑–≤–ª–µ—á–µ—Ç ID –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏\n\n")
        f.write("\n".join(sample_links))
    print("  –°–æ–∑–¥–∞–Ω: –ø—Ä–∏–º–µ—Ä_links.txt (3 —Å—Å—ã–ª–∫–∏)")
    
    # 3. –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–æ–Ω–Ω—ã–π —Ñ–∞–π–ª
    config_sample = {
        "–∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è": "–≠—Ç–æ –ø—Ä–∏–º–µ—Ä –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏",
        "—Ç–æ–∫–µ–Ω": "–í–∞—à_—Ç–æ–∫–µ–Ω_–∑–¥–µ—Å—å",
        "–ª–∏–º–∏—Ç_–∑–∞–ø–∏—Å–µ–π": 1000,
        "—Å–æ—Ö—Ä–∞–Ω—è—Ç—å_–∫–∞–∂–¥—ã–µ": 100
    }
    with open("–ø—Ä–∏–º–µ—Ä_config.json", "w", encoding="utf-8") as f:
        json.dump(config_sample, f, ensure_ascii=False, indent=2)
    print("  –°–æ–∑–¥–∞–Ω: –ø—Ä–∏–º–µ—Ä_config.json")

def print_help():
    """–í—ã–≤–æ–¥ —Å–ø—Ä–∞–≤–∫–∏"""
    help_text = """
–ü–ê–†–°–ï–† –†–ï–ï–°–¢–†–ê –§–°–ê - –°–ü–†–ê–í–ö–ê

–ò–°–ü–û–õ–¨–ó–û–í–ê–ù–ò–ï:
  python fsa_parser.py [–∫–æ–º–∞–Ω–¥–∞]

–ö–û–ú–ê–ù–î–´:
  run      - –ó–∞–ø—É—Å–∫ –ø–æ–ª–Ω–æ–≥–æ –ø–∞—Ä—Å–∏–Ω–≥–∞
  test     - –¢–µ—Å—Ç–æ–≤—ã–π –∑–∞–ø—É—Å–∫ (10 –∑–∞–ø–∏—Å–µ–π)
  check    - –ü—Ä–æ–≤–µ—Ä–∫–∞ –æ–∫—Ä—É–∂–µ–Ω–∏—è –∏ API
  sample   - –°–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–∏–º–µ—Ä–æ–≤ —Ñ–∞–π–ª–æ–≤
  help     - –≠—Ç–∞ —Å–ø—Ä–∞–≤–∫–∞

–ü–û–î–ì–û–¢–û–í–ö–ê:
  1. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –æ–∫—Ä—É–∂–µ–Ω–∏–µ: python fsa_parser.py check
  2. –°–æ–∑–¥–∞–π—Ç–µ —Ñ–∞–π–ª —Å ID –∫–æ–º–ø–∞–Ω–∏–π (company_ids.txt)
  3. –ó–∞–ø—É—Å—Ç–∏—Ç–µ —Ç–µ—Å—Ç: python fsa_parser.py test
  4. –ó–∞–ø—É—Å—Ç–∏—Ç–µ –ø–æ–ª–Ω—ã–π –ø–∞—Ä—Å–µ—Ä: python fsa_parser.py run

–§–ê–ô–õ–´:
  - company_ids.txt - —Å–ø–∏—Å–æ–∫ ID (–ø–æ –æ–¥–Ω–æ–º—É –Ω–∞ —Å—Ç—Ä–æ–∫—É)
  - links.txt - —Å–ø–∏—Å–æ–∫ —Å—Å—ã–ª–æ–∫ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –∫–æ–º–ø–∞–Ω–∏–π
  - ids.txt - –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ —Ñ–∞–π–ª–∞ —Å ID

–í–´–•–û–î–ù–´–ï –§–ê–ô–õ–´:
  - —Ä–µ–µ—Å—Ç—Ä_—Ñ—Å–∞_–ø–æ–ª–Ω—ã–π.xlsx - –æ—Å–Ω–æ–≤–Ω–æ–π —Ñ–∞–π–ª Excel
  - —Ä–µ–µ—Å—Ç—Ä_—Ñ—Å–∞_–ø–æ–ª–Ω—ã–π.csv - —Ä–µ–∑–µ—Ä–≤–Ω—ã–π CSV
  - —Ä–µ–µ—Å—Ç—Ä_—Ñ—Å–∞_–ø–æ–ª–Ω—ã–π.json - —Ä–µ–∑–µ—Ä–≤–Ω—ã–π JSON
  - fsa_parser_full.log - —Ñ–∞–π–ª –ª–æ–≥–æ–≤
  - –æ—Ç—á–µ—Ç_–æ_–ø–∞—Ä—Å–∏–Ω–≥–µ.txt - —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞

–ü–†–ò–ú–ï–ß–ê–ù–ò–Ø:
  - –î–ª—è 38000 –∑–∞–ø–∏—Å–µ–π –ø–æ—Ç—Ä–µ–±—É–µ—Ç—Å—è ~2-3 —á–∞—Å–∞
  - –¢–æ–∫–µ–Ω –∞–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏ –º–æ–∂–µ—Ç –ø–æ—Ç—Ä–µ–±–æ–≤–∞—Ç—å –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è
  - –ü–∞—Ä—Å–µ—Ä –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    """
    print(help_text)

# ================= –¢–û–ß–ö–ê –í–•–û–î–ê =================
async def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    
    if len(sys.argv) > 1:
        command = sys.argv[1].lower()
    else:
        command = "help"
    
    if command == "run":
        # –ü–æ–ª–Ω—ã–π –∑–∞–ø—É—Å–∫
        parser = FSAParser()
        await parser.run()
        
    elif command == "test":
        # –¢–µ—Å—Ç–æ–≤—ã–π –∑–∞–ø—É—Å–∫
        print("üß™ –¢–ï–°–¢–û–í–´–ô –ó–ê–ü–£–°–ö (10 –∑–∞–ø–∏—Å–µ–π)")
        CONFIG["max_records"] = 10
        CONFIG["output_excel"] = "—Ç–µ—Å—Ç_—Ä–µ–µ—Å—Ç—Ä.xlsx"
        
        # –°–æ–∑–¥–∞–µ–º —Ç–µ—Å—Ç–æ–≤—ã–µ ID
        test_ids = [str(i) for i in range(1, 11)]
        with open("company_ids.txt", "w", encoding="utf-8") as f:
            f.write("\n".join(test_ids))
        print("–°–æ–∑–¥–∞–Ω company_ids.txt —Å 10 ID")
        
        parser = FSAParser()
        await parser.run()
        
    elif command == "check":
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –æ–∫—Ä—É–∂–µ–Ω–∏—è
        check_environment()
        
    elif command == "sample":
        # –°–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–∏–º–µ—Ä–æ–≤ —Ñ–∞–π–ª–æ–≤
        create_sample_files()
        
    elif command == "help":
        # –°–ø—Ä–∞–≤–∫–∞
        print_help()
        
    else:
        print(f"–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è –∫–æ–º–∞–Ω–¥–∞: {command}")
        print("–ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ: python fsa_parser.py help")

if __name__ == "__main__":
    # –ó–∞–ø—É—Å–∫ –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–≥–æ –∫–æ–¥–∞
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        print("\n\n‚ö†Ô∏è  –ü–∞—Ä—Å–µ—Ä –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º")
    except Exception as e:
        print(f"\n\n‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {e}")
        print("–ü–æ–¥—Ä–æ–±–Ω–æ—Å—Ç–∏ –≤ –ª–æ–≥-—Ñ–∞–π–ª–µ")
